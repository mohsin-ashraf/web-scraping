{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mechanize\n",
    "import lxml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = mechanize.Browser()#br short for browser\n",
    "br.set_handle_robots(False)\n",
    "br.addheaders = [(\"user-agent\",\"Chrome\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safeStr(obj):\n",
    "    try: return str(obj)\n",
    "    except UnicodeEncodeError:\n",
    "        return obj.encode('ascii', 'ignore').decode('ascii')\n",
    "    except: return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://elitedatascience.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_page = br.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print html_page.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_page = html_page.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = lxml.html.fromstring(html_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = \"//h1[@class='op-list-headline']//a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_posts = tree.xpath(xpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_posts = [x.attrib['href'] for x in links_to_posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://elitedatascience.com/python-quickstart\n",
      "https://elitedatascience.com/python-data-wrangling-tutorial\n",
      "https://elitedatascience.com/become-a-data-scientist\n",
      "https://elitedatascience.com/python-cheat-sheet\n",
      "https://elitedatascience.com/open-source-vs-commercial-ml-software\n",
      "https://elitedatascience.com/resume-tips\n",
      "https://elitedatascience.com/machine-learning-interview-questions-answers\n",
      "https://elitedatascience.com/machine-learning-projects-for-beginners\n",
      "https://elitedatascience.com/overfitting-in-machine-learning\n"
     ]
    }
   ],
   "source": [
    "for link in links_to_posts:\n",
    "    print link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_post = br.open(\"https://elitedatascience.com/python-data-wrangling-tutorial\").read()\n",
    "tree = lxml.html.fromstring(blog_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python Data Wrangling Tutorial: Cryptocurrency Edition'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_title = tree.cssselect('h1.op-headline')\n",
    "post_title = post_title[0]\n",
    "post_title.text_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'58SHARESShareGoogleLinkedinTweet\\r\\n            \\r\\n                \\r\\n            Bitcoin and cryptocurrency have been all the rage\\u2026 but as data scientists, we\\u2019re empiricists, right? We don\\u2019t want to just take others\\u2019 word for it\\u2026 we want to look at the data firsthand! In this tutorial, we\\u2019ll introduce common and powerful techniques for data wrangling in Python.\\nBroadly speaking, data wrangling is the process of reshaping, aggregating, separating, or otherwise transforming your data from one format to a more useful one.\\nFor example,. let\\u2019s say we wanted to run a step-forward analysis of a very rudimentary momentum trading strategy that goes as follows:\\n\\nAt the start of every month, we buy the cryptocurrency that had the largest price gain over the previous 7, 14, 21, or 28 days. We want to evaluate each of these time windows.\\nThen, we hold for exactly 7 days and sell our position.\\xa0Please note: this is a purposefully simple strategy that is only meant for illustrative purposes.\\n\\nHow well would we go about evaluating this strategy?\\nThis is a great question for showcasing data wrangling techniques because all the hard work lies in molding your dataset into the proper format. Once you have the appropriate analytical base table (ABT), answering the question becomes simple.\\nWhat this guide is not:\\nThis is not a guide about investment or trading strategies, nor is it an endorsement for or against cryptocurrency. Potential investors should form their own views independently, but this guide will introduce tools for doing so.\\nAgain, the focus of this tutorial is on data wrangling techniques and the ability to transform raw datasets into formats that help you answer interesting questions.\\nA quick tip before we begin:\\nThis tutorial is designed to be\\xa0streamlined, and it won\\u2019t cover any\\xa0one topic in too much\\xa0detail. It may be helpful to have the\\xa0Pandas library documentation\\xa0open beside you\\xa0as a supplemental\\xa0reference.\\nPython Data Wrangling Tutorial Contents\\nHere are the steps we\\u2019ll take for our analysis:\\n\\nSet up your environment.\\nImport libraries and dataset.\\nUnderstand the data.\\nFilter unwanted observations.\\nPivot the dataset.\\nShift the pivoted dataset.\\nMelt the shifted dataset.\\nReduce-merge the melted data.\\nAggregate with group-by.\\n\\n\\nStep 1: Set up your environment.\\nFirst, make sure you have the following installed on your computer:\\n\\nPython 2.7+ or Python 3\\nPandas\\nJupyter Notebook (optional, but recommended)\\n\\nWe strongly recommend installing the Anaconda Distribution, which comes with all of those packages. Simply follow the instructions on that download page.\\nOnce you have Anaconda installed, simply start\\xa0Jupyter (either through the command line or the Navigator app) and open a new notebook:\\n\\n                \\n            \\nPython 3 or Python 2.7+ are both fine.\\nStep 2: Import libraries and dataset.\\nLet\\'s start by importing Pandas, the best Python library for wrangling relational (i.e. table-format) datasets. Pandas will be doing most of the heavy lifting for this tutorial.\\n\\nTip:\\xa0we\\'ll give Pandas an\\xa0alias. Later,\\xa0we can invoke the library with \\r\\n\\t\\t\\tpd.\\n\\n\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tPandas\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t12\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Pandas for managing datasetsimport pandas as pd\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\tNext, let\\'s tweak the display options a bit. First, let\\'s display floats with 2 decimal places to make tables less crowded. Don\\'t worry... this is only a display setting that doesn\\'t reduce the underlying precision. Let\\'s also expand the limits for the number of rows and columns displayed.\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tPandas display settings\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t123456\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Display floats with 2 decimal placespd.options.display.float_format = \\'{:,.2f}\\'.format\\xa0# Expand display limitspd.options.display.max_rows = 200pd.options.display.max_columns = 100\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\tFor this tutorial, we\\'ll be using a price dataset managed by Brave New Coin and distributed on Quandl. The full version tracks price indices for 1,900+ fiat-crypto trading pairs, but it requires a premium subscription, so we\\'ve provided a small sample with a handful of cryptocurrencies.\\nTo follow along, you can download BNC2_sample.csv. Clicking that link will take you to Google Drive, and then simply click the download icon in the top right:\\n\\n                \\n            \\nOnce you\\'ve downloaded the dataset and put in the same file directory as your Jupyter notebook, you can run the following code to read the dataset into a Pandas dataframe and display example observations.\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tRead sample dataset\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t1234567\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Read BNC2 sample datasetdf = pd.read_csv(\\'BNC2_sample.csv\\',\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 names=[\\'Code\\', \\'Date\\', \\'Open\\', \\'High\\', \\'Low\\', \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\'Close\\', \\'Volume\\', \\'VWAP\\', \\'TWAP\\'])\\xa0# Display first 5 observationsdf.head()\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\n            \\nNote that we use the\\xa0\\r\\n\\t\\t\\tnames=\\xa0argument for\\xa0\\r\\n\\t\\t\\tpd.read_csv()\\xa0to set our own column names because the original dataset does not have any.\\nData Dictionary (for code\\xa0GWA_BTC):\\n\\nDate:\\xa0The day on which the index values were calculated.\\nOpen:\\xa0The day\\'s opening price index for Bitcoin in US dollars.\\nHigh:\\xa0The highest value for the price index for Bitcoin in US dollars that day.\\nLow:\\xa0The lowest value for the price index for Bitcoin in US dollars that day.\\nClose:\\xa0The day\\'s closing price index for Bitcoin in US dollars.\\nVolume:\\xa0The volume of Bitcoin traded that day.\\nVWAP:\\xa0The volume weighted average price of Bitcoin traded that day.\\nTWAP:\\xa0The time-weighted average price of Bitcoin traded that day.\\n\\nStep 3: Understand the data.\\nOne of the most common reasons to wrangle data is when there\\'s \"too much\" information packed into a single table, especially when dealing with time series data.\\nGenerally, all observations should be equivalent in granularity and in\\xa0units.\\nThere will be exceptions, but for the most part, this rule of thumb can save you from many headaches.\\n\\nEquivalence in Granularity -\\xa0For example, you could have 10 rows of data from 10 different cryptocurrencies. However, you should\\xa0not\\xa0have an 11th row with average or total values from the other 10 rows. That 11th row would be an aggregation, and thus not equivalent in granularity to the other 10.\\nEquivalence in Units - You could have 10 rows with prices in USD collected at different dates. However, you should\\xa0not\\xa0then have another 10 rows with prices quoted in EUR. Any aggregations, distributions, visualizations, or statistics would become meaningless.\\n\\nOur current raw dataset breaks both of these rules!\\nData stored in CSV files or databases are often in \\u201cstacked\\u201d or \\u201crecord\\u201d format. They use a single\\xa0\\r\\n\\t\\t\\t\\'Code\\'\\xa0column as a catch-all for metadata. For example, in the sample dataset, we have the follow codes:\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tUnique codes in the dataset\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t123456789\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Unique codes in the datasetprint( df.Code.unique() )\\xa0# [\\'GWA_BTC\\' \\'GWA_ETH\\' \\'GWA_LTC\\' \\'GWA_XLM\\' \\'GWA_XRP\\' \\'MWA_BTC_CNY\\'#\\xa0\\xa0\\'MWA_BTC_EUR\\' \\'MWA_BTC_GBP\\' \\'MWA_BTC_JPY\\' \\'MWA_BTC_USD\\' \\'MWA_ETH_CNY\\'#\\xa0\\xa0\\'MWA_ETH_EUR\\' \\'MWA_ETH_GBP\\' \\'MWA_ETH_JPY\\' \\'MWA_ETH_USD\\' \\'MWA_LTC_CNY\\'#\\xa0\\xa0\\'MWA_LTC_EUR\\' \\'MWA_LTC_GBP\\' \\'MWA_LTC_JPY\\' \\'MWA_LTC_USD\\' \\'MWA_XLM_CNY\\'#\\xa0\\xa0\\'MWA_XLM_EUR\\' \\'MWA_XLM_USD\\' \\'MWA_XRP_CNY\\' \\'MWA_XRP_EUR\\' \\'MWA_XRP_GBP\\'#\\xa0\\xa0\\'MWA_XRP_JPY\\' \\'MWA_XRP_USD\\']\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\tFirst, see how some codes start with GWA and others with MWA? These are actually completely different types of indicators according to the documentation page.\\n\\nMWA stands for \"market-weighted average,\" and they show regional prices. There are multiple MWA codes for each cryptocurrency, one for each local fiat currency.\\nOn the other hand, GWA stands for \"global-weighted average,\" which shows globally indexed prices. GWA is thus an aggregation of MWA and not equivalent in granularity. (Note: only a subset of regional MWA codes are included in the sample dataset.)\\n\\nFor instance, let\\'s look at Bitcoin\\'s codes on the same date:\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tExample of GWA and MWA relationship\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t123\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Example of GWA and MWA relationshipdf[df.Code.isin([\\'GWA_BTC\\', \\'MWA_BTC_JPY\\', \\'MWA_BTC_EUR\\']) \\xa0\\xa0 & (df.Date == \\'2018-01-01\\')]\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\n            \\nAs you can see, we have multiple entries for a cryptocurrency on a given date. To further complicate things, the regional MWA data are denominated in their local currency (i.e. nonequivalent units), so you would also need historical exchange rates.\\nHaving different levels of granularity and/or different units makes analysis unwieldy at best, or downright impossible at worst.\\nLuckily, once we\\'ve spotted this issue, fixing it is actually trivial!\\nStep 4: Filter unwanted observations.\\nOne of the simplest yet most useful data wrangling techniques is removing unwanted observations.\\nIn the previous step, we learned that GWA codes are aggregations of the regional MWA codes. Therefore, to perform our analysis, we only need to keep the global GWA codes:\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tFilter out MWA codes\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t12345678910111213\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Number of observations in datasetprint( \\'Before:\\', len(df) )# Before: 31761\\xa0# Get all the GWA codesgwa_codes = [code for code in df.Code.unique() if \\'GWA_\\' in code]\\xa0# Only keep GWA observationsdf = df[df.Code.isin(gwa_codes)]\\xa0# Number of observations leftprint( \\'After:\\', len(df) )# After: 6309\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\tNow that we only have GWA codes left, all of our observations are equivalent in granularity and in units. We can confidently proceed.\\nStep 5: Pivot the dataset.\\nNext, in order to analyze our momentum trading strategy outlined above, for each cryptocurrency, we\\'ll need calculate returns over the prior 7, 14, 21, and 28 days... for the first day of each month.\\nHowever, it would be a huge pain to do so with the current \"stacked\" dataset. It would involve writing helper functions, loops, and plenty of conditional logic. Instead, we\\'ll take a more elegant approach....\\nFirst, we\\'ll pivot the dataset while keeping only one price column. For this tutorial, let\\'s keep the\\xa0VWAP (volume weighted average price) column, but you could make a good case for most of them.\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tPivot dataset\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t12345\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Pivot datasetpivoted_df = df.pivot(index=\\'Date\\', columns=\\'Code\\', values=\\'VWAP\\')\\xa0# Display examples from pivoted datasetpivoted_df.tail()\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\n            \\nAs you can see, each column in our pivoted dataset now represents the price for one cryptocurrency and each row contains prices from one date. All the features are now aligned by date.\\nStep 6: Shift the pivoted dataset.\\nTo easily calculate returns over the prior 7, 14, 21, and 28 days, we can use Pandas\\'s shift\\xa0method.\\nThis function\\xa0shifts the index\\xa0of the dataframe by some number of periods. For example, here\\'s what happens when we shift our pivoted dataset by 1:\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tShift method\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t12345678910111213\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tprint( pivoted_df.tail(3) )# Code\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 GWA_BTC\\xa0\\xa0GWA_ETH\\xa0\\xa0GWA_LTC\\xa0\\xa0GWA_XLM\\xa0\\xa0GWA_XRP# Date\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0# 2018-01-21 12,326.23 1,108.90\\xa0\\xa0 197.36\\xa0\\xa0\\xa0\\xa0 0.48\\xa0\\xa0\\xa0\\xa0 1.55# 2018-01-22 11,397.52 1,038.21\\xa0\\xa0 184.92\\xa0\\xa0\\xa0\\xa0 0.47\\xa0\\xa0\\xa0\\xa0 1.43# 2018-01-23 10,921.00\\xa0\\xa0 992.05\\xa0\\xa0 176.95\\xa0\\xa0\\xa0\\xa0 0.47\\xa0\\xa0\\xa0\\xa0 1.42\\xa0print( pivoted_df.tail(3).shift(1) )# Code\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 GWA_BTC\\xa0\\xa0GWA_ETH\\xa0\\xa0GWA_LTC\\xa0\\xa0GWA_XLM\\xa0\\xa0GWA_XRP# Date\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0# 2018-01-21\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 nan\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nan\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nan\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nan\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nan# 2018-01-22 12,326.23 1,108.90\\xa0\\xa0 197.36\\xa0\\xa0\\xa0\\xa0 0.48\\xa0\\xa0\\xa0\\xa0 1.55# 2018-01-23 11,397.52 1,038.21\\xa0\\xa0 184.92\\xa0\\xa0\\xa0\\xa0 0.47\\xa0\\xa0\\xa0\\xa0 1.43\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\tNotice how the shifted dataset now has values from 1 day before? We can take advantage of this to calculate prior returns for our 7, 14, 21, 28 day windows.\\nFor example, to calculate returns over the 7 days prior, we would need\\xa0\\r\\n\\t\\t\\tprices_today / prices_7_days_ago - 1.0, which translates to:\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tCalculate returns over 7 days prior\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t12345\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Calculate returns over 7 days priordelta_7 = pivoted_df / pivoted_df.shift(7) - 1.0\\xa0# Display examplesdelta_7.tail()\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\n            \\nCalculating returns for all of our windows is as easy as writing a loop and storing them in a dictionary:\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tLoop over time windows\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t1234\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Calculate returns over each window and store them in dictionarydelta_dict = {}for offset in [7, 14, 21, 28]:\\xa0\\xa0\\xa0\\xa0delta_dict[\\'delta_{}\\'.format(offset)] = pivoted_df / pivoted_df.shift(offset) - 1.0\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\tNote: Calculating returns by shifting the dataset requires 2 assumptions to be met: (1) the observations are sorted ascending by date and (2) there are no missing dates. We checked this \"off-stage\" to keep this tutorial concise, but we recommend confirming this on your own.\\nStep 7: Melt the shifted dataset.\\nNow that we\\'ve calculated returns using the pivoted dataset, we\\'re going to \"unpivot\" the returns. By unpivoting, or\\xa0melting\\xa0the data, we can later create an analytical base table (ABT)\\xa0where each row contains all of the relevant information for a particular coin on a particular date.\\nWe couldn\\'t directly shift the original dataset because the data for different coins were stacked on each other, so the boundaries would\\'ve overlapped. In other words, BTC data would leak into ETH calculations, ETH data would leak into LTC calculations, and so on.\\nTo melt the data, we\\'ll...\\n\\n\\r\\n\\t\\t\\treset_index()\\xa0so we can call the columns by name.\\nCall the\\xa0\\r\\n\\t\\t\\tmelt()\\xa0method.\\nPass the column(s) to keep into the\\xa0\\r\\n\\t\\t\\tid_vars=\\xa0argument.\\nName the melted column using the\\xa0\\r\\n\\t\\t\\tvalue_name=\\xa0argument.\\n\\nHere\\'s how that looks for one dataframe:\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tMelt shifted data\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t12345\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Melt delta_7 returnsmelted_7 = delta_7.reset_index().melt(id_vars=[\\'Date\\'], value_name=\\'delta_7\\')\\xa0# Melted dataframe examplesmelted_7.tail()\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\n            \\nTo do so for all of the returns dataframes, we can simply loop through\\xa0\\r\\n\\t\\t\\tdelta_dict, like so:\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\t# Melt all the delta dataframes\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t1234\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Melt all the delta dataframes and store in listmelted_dfs = []for key, delta_df in delta_dict.items():\\xa0\\xa0\\xa0\\xa0melted_dfs.append( delta_df.reset_index().melt(id_vars=[\\'Date\\'], value_name=key) )\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\tFinally, we can create another melted dataframe that contains the forward-looking 7-day returns. This will be our \"target variable\" for evaluating our trading strategy.\\nSimply shift the pivoted dataset by \\r\\n\\t\\t\\t-7\\xa0 to get \"future\" prices, like so:\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tCalculate forward-looking 7-day returns\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t12345\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Calculate 7-day returns after the datereturn_df = pivoted_df.shift(-7) / pivoted_df - 1.0\\xa0# Melt the return dataset and append to listmelted_dfs.append( return_df.reset_index().melt(id_vars=[\\'Date\\'], value_name=\\'return_7\\') )\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\tWe now have 5 melted dataframes stored in the\\xa0\\r\\n\\t\\t\\tmelted_dfs\\xa0list, one for each of the backward-looking 7, 14, 21, and 28-day returns and one for the forward-looking 7-day returns.\\nStep 8: Reduce-merge the melted data.\\nAll that\\'s left to do is join our melted dataframes into a single analytical base table. We\\'ll need two tools.\\nThe first is Pandas\\'s merge function, which works like SQL JOIN. For example, to merge the first two melted dataframes...\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tMerge two dataframes\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t12\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Merge two dataframespd.merge(melted_dfs[0], melted_dfs[1], on=[\\'Date\\', \\'Code\\']).tail()\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\n            \\nSee how we now have\\xa0delta_7 and\\xa0delta_14 in the same row? This is the start of our analytical base table. All we need to do now is merge all of our melted dataframes together with a base dataframe of other features we might want.\\nThe most elegant way to do this is using Python\\'s built-in\\xa0reduce function. First we\\'ll need to import it:\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tImport reduce function\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t1\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tfrom functools import reduce\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\tNext, before we use that function, let\\'s create a\\xa0\\r\\n\\t\\t\\tfeature_dfs\\xa0list that contains base features from the original dataset plus the melted datasets.\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tCreate feature_dfs list\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t12345\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Grab features from original datasetbase_df = df[[\\'Date\\', \\'Code\\', \\'Volume\\', \\'VWAP\\']]\\xa0# Create a list with all the feature dataframesfeature_dfs = [base_df] + melted_dfs\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\tNow we\\'re ready to use the reduce function. Reduce applies a function of two arguments cumulatively to the objects in a sequence (e.g. a list). For example,\\xa0\\r\\n\\t\\t\\treduce(lambda x,y: x+y, [1,2,3,4,5])\\xa0calculates\\xa0\\r\\n\\t\\t\\t((((1+2)+3)+4)+5).\\nThus, we can reduce-merge all of the features like so:\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tReduce-merge features into ABT\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t12345\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Reduce-merge features into analytical base tableabt = reduce(lambda left,right: pd.merge(left,right,on=[\\'Date\\', \\'Code\\']), feature_dfs)\\xa0# Display examples from the ABTabt.tail(10)\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\n            \\nData Dictionary for our Analytical Base Table (ABT):\\n\\nDate:\\xa0The day on which the index values were calculated.\\nCode:\\xa0Which cryptocurrency.\\nVWAP:\\xa0The volume weighted average price traded that day.\\ndelta_7:\\xa0Return over the prior 7 days (1.0 = 100% return).\\ndelta_14:\\xa0Return over the prior 14 days (1.0 = 100% return).\\ndelta_21:\\xa0Return over the prior 21 days (1.0 = 100% return).\\ndelta_28:\\xa0Return over the prior 28 days (1.0 = 100% return).\\nreturn_7:\\xa0Future return over the next 7 days (1.0 = 100% return).\\n\\nBy the way, notice how the last 7 observations don\\'t have values for the\\xa0\\r\\n\\t\\t\\t\\'return_7\\'\\xa0feature? This is expected, as we cannot calculate \"future 7-day returns\" for the last 7 days of the dataset.\\nTechnically, with this ABT, we can already answer our original objective. For example, if we wanted to pick the coin that had the biggest momentum on September 1st, 2017, we could simply display the rows for that date and look at the 7, 14, 21, and 28-day prior returns:\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\t# Data from Sept 1st, 2017\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t12\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Data from Sept 1st, 2017abt[abt.Date == \\'2017-09-01\\']\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\n            \\nAnd if you wanted to programmatically pick the crypto with the biggest momentum (e.g. over the prior 28 days), you would write:\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tProgrammatically pick highest momentum\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t12345\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tmax_momentum_id = abt[abt.Date == \\'2017-09-01\\'].delta_28.idxmax()daily_df.loc[max_momentum_id, [\\'Code\\',\\'return_7\\']]# Code\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0GWA_LTC# return_7\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0-0.10# Name: 3543, dtype: object\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\tHowever, since we\\'re only interested in trading on the first day of each month, we can make things even easier for ourselves...\\nStep 9: (Optional) Aggregate with group-by.\\nAs a final step, if we wanted to only keep the first days of each month, we can use a\\xa0group-by\\xa0followed by an aggregation.\\n\\nFirst, create a new \\r\\n\\t\\t\\t\\'month\\'\\xa0 feature from the first 7 characters of the Date strings.\\nThen, group the observations by\\xa0\\r\\n\\t\\t\\t\\'Code\\'\\xa0and by\\xa0\\r\\n\\t\\t\\t\\'month\\'. Pandas will create \"cells\" of data that separate observations by Code and month.\\nFinally, within each group, simply take the\\xa0\\r\\n\\t\\t\\t.first()\\xa0observation and reset the index.\\n\\nNote: We\\'re assuming your dataframe is still properly sorted by date.\\nHere\\'s what it looks like all put together:\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tAggregate with group-by\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t12345678\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# Create \\'month\\' featureabt[\\'month\\'] = abt.Date.apply(lambda x: x[:7])\\xa0# Group by \\'Code\\' and \\'month\\' and keep first dategb_df = abt.groupby([\\'Code\\', \\'month\\']).first().reset_index()\\xa0# Display examplesgb_df.tail()\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\n            \\nAs you can see, we now have a proper ABT with:\\n\\nOnly relevant data from the 1st day of each month.\\nMomentum features calculated from the prior 7, 14, 21, and 28 days.\\nThe future returns you would\\'ve made 7 days later.\\n\\nIn other words, we have exactly what we need to evaluate the simple trading strategy we proposed at the beginning!\\n\\nCongratulations... you\\'ve made it to the end of this Python data wrangling tutorial!\\nWe introduced several key tools for filtering, manipulating, and transforming datasets in Python, but we\\'ve only scratched the surface. Pandas is a very powerful library with plenty of additional functionality.\\nFor continued learning, we recommend downloading more datasets\\xa0for hands-on practice. Propose an interesting question, plan your approach, and fall back on documentation for help.\\nWe also provide over-the-shoulder guidance in our popular Machine Learning Masterclass.\\xa0It\\'s a hands-on course developed completely in-house... and it\\'s designed take you from 0 to machine learning as smoothly as possible (without the boring lectures).\\nThe complete code, from start to finish.\\nHere\\'s\\xa0all the main code in one place, in a single script.\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tComplete script\\r\\n\\t\\t\\tPython\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t1234567891011121314151617181920212223242526272829303132333435363738394041\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t# 2. Import libraries and datasetimport pandas as pdpd.options.display.float_format = \\'{:,.2f}\\'.formatpd.options.display.max_rows = 200pd.options.display.max_columns = 100\\xa0df = pd.read_csv(\\'BNC2_sample.csv\\',\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 names=[\\'Code\\', \\'Date\\', \\'Open\\', \\'High\\', \\'Low\\', \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\'Close\\', \\'Volume\\', \\'VWAP\\', \\'TWAP\\'])\\xa0# 4. Filter unwanted observationsgwa_codes = [code for code in df.Code.unique() if \\'GWA_\\' in code]df = df[df.Code.isin(gwa_codes)]\\xa0# 5. Pivot the datasetpivoted_df = df.pivot(index=\\'Date\\', columns=\\'Code\\', values=\\'VWAP\\')\\xa0# 6. Shift the pivoted datasetdelta_dict = {}for offset in [7, 14, 21, 28]:\\xa0\\xa0\\xa0\\xa0delta_dict[\\'delta_{}\\'.format(offset)] = pivoted_df / pivoted_df.shift(offset) - 1\\xa0\\xa0\\xa0\\xa0# 7. Melt the shifted datasetmelted_dfs = []for key, delta_df in delta_dict.items():\\xa0\\xa0\\xa0\\xa0melted_dfs.append( delta_df.reset_index().melt(id_vars=[\\'Date\\'], value_name=key) )\\xa0return_df = pivoted_df.shift(-7) / pivoted_df - 1.0melted_dfs.append( return_df.reset_index().melt(id_vars=[\\'Date\\'], value_name=\\'return_7\\') )\\xa0# 8. Reduce-merge the melted datafrom functools import reduce\\xa0base_df = df[[\\'Date\\', \\'Code\\', \\'Volume\\', \\'VWAP\\']]feature_dfs = [base_df] + melted_dfs\\xa0abt = reduce(lambda left,right: pd.merge(left,right,on=[\\'Date\\', \\'Code\\']), feature_dfs)\\xa0# 9. Aggregate with group-by.abt[\\'month\\'] = abt.Date.apply(lambda x: x[:7])gb_df = abt.groupby([\\'Code\\', \\'month\\']).first().reset_index()\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\n58SHARESShareGoogleLinkedinTweet\\r\\n            \\r\\n                \\r\\n            \\n\\n\\n                \\n                    \\n                    \\n                        \\n                            \\xab Previous Post\\n                            How to Become a Data Scientist, The Self-Starter Way\\n                        \\n                        \\n                            Next Post \\xbb\\n                            Python for Data Science (Ultimate Quickstart Guide)'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_content = tree.cssselect('div.col-sm-12.col-md-8.col-md-offset-2')[0]\n",
    "post_content.text_content().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58SHARESShareGoogleLinkedinTweet\r\n",
      "            \r\n",
      "                \r\n",
      "            Bitcoin and cryptocurrency have been all the rage but as data scientists, were empiricists, right? We dont want to just take others word for it we want to look at the data firsthand! In this tutorial, well introduce common and powerful techniques for data wrangling in Python.\n",
      "Broadly speaking, data wrangling is the process of reshaping, aggregating, separating, or otherwise transforming your data from one format to a more useful one.\n",
      "For example,. lets say we wanted to run a step-forward analysis of a very rudimentary momentum trading strategy that goes as follows:\n",
      "\n",
      "At the start of every month, we buy the cryptocurrency that had the largest price gain over the previous 7, 14, 21, or 28 days. We want to evaluate each of these time windows.\n",
      "Then, we hold for exactly 7 days and sell our position.Please note: this is a purposefully simple strategy that is only meant for illustrative purposes.\n",
      "\n",
      "How well would we go about evaluating this strategy?\n",
      "This is a great question for showcasing data wrangling techniques because all the hard work lies in molding your dataset into the proper format. Once you have the appropriate analytical base table (ABT), answering the question becomes simple.\n",
      "What this guide is not:\n",
      "This is not a guide about investment or trading strategies, nor is it an endorsement for or against cryptocurrency. Potential investors should form their own views independently, but this guide will introduce tools for doing so.\n",
      "Again, the focus of this tutorial is on data wrangling techniques and the ability to transform raw datasets into formats that help you answer interesting questions.\n",
      "A quick tip before we begin:\n",
      "This tutorial is designed to bestreamlined, and it wont cover anyone topic in too muchdetail. It may be helpful to have thePandas library documentationopen beside youas a supplementalreference.\n",
      "Python Data Wrangling Tutorial Contents\n",
      "Here are the steps well take for our analysis:\n",
      "\n",
      "Set up your environment.\n",
      "Import libraries and dataset.\n",
      "Understand the data.\n",
      "Filter unwanted observations.\n",
      "Pivot the dataset.\n",
      "Shift the pivoted dataset.\n",
      "Melt the shifted dataset.\n",
      "Reduce-merge the melted data.\n",
      "Aggregate with group-by.\n",
      "\n",
      "\n",
      "Step 1: Set up your environment.\n",
      "First, make sure you have the following installed on your computer:\n",
      "\n",
      "Python 2.7+ or Python 3\n",
      "Pandas\n",
      "Jupyter Notebook (optional, but recommended)\n",
      "\n",
      "We strongly recommend installing the Anaconda Distribution, which comes with all of those packages. Simply follow the instructions on that download page.\n",
      "Once you have Anaconda installed, simply startJupyter (either through the command line or the Navigator app) and open a new notebook:\n",
      "\n",
      "                \n",
      "            \n",
      "Python 3 or Python 2.7+ are both fine.\n",
      "Step 2: Import libraries and dataset.\n",
      "Let's start by importing Pandas, the best Python library for wrangling relational (i.e. table-format) datasets. Pandas will be doing most of the heavy lifting for this tutorial.\n",
      "\n",
      "Tip:we'll give Pandas analias. Later,we can invoke the library with \r\n",
      "\t\t\tpd.\n",
      "\n",
      "\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tPandas\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t12\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Pandas for managing datasetsimport pandas as pd\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\tNext, let's tweak the display options a bit. First, let's display floats with 2 decimal places to make tables less crowded. Don't worry... this is only a display setting that doesn't reduce the underlying precision. Let's also expand the limits for the number of rows and columns displayed.\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tPandas display settings\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t123456\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Display floats with 2 decimal placespd.options.display.float_format = '{:,.2f}'.format# Expand display limitspd.options.display.max_rows = 200pd.options.display.max_columns = 100\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\tFor this tutorial, we'll be using a price dataset managed by Brave New Coin and distributed on Quandl. The full version tracks price indices for 1,900+ fiat-crypto trading pairs, but it requires a premium subscription, so we've provided a small sample with a handful of cryptocurrencies.\n",
      "To follow along, you can download BNC2_sample.csv. Clicking that link will take you to Google Drive, and then simply click the download icon in the top right:\n",
      "\n",
      "                \n",
      "            \n",
      "Once you've downloaded the dataset and put in the same file directory as your Jupyter notebook, you can run the following code to read the dataset into a Pandas dataframe and display example observations.\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tRead sample dataset\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t1234567\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Read BNC2 sample datasetdf = pd.read_csv('BNC2_sample.csv', names=['Code', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'TWAP'])# Display first 5 observationsdf.head()\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\n",
      "            \n",
      "Note that we use the\r\n",
      "\t\t\tnames=argument for\r\n",
      "\t\t\tpd.read_csv()to set our own column names because the original dataset does not have any.\n",
      "Data Dictionary (for codeGWA_BTC):\n",
      "\n",
      "Date:The day on which the index values were calculated.\n",
      "Open:The day's opening price index for Bitcoin in US dollars.\n",
      "High:The highest value for the price index for Bitcoin in US dollars that day.\n",
      "Low:The lowest value for the price index for Bitcoin in US dollars that day.\n",
      "Close:The day's closing price index for Bitcoin in US dollars.\n",
      "Volume:The volume of Bitcoin traded that day.\n",
      "VWAP:The volume weighted average price of Bitcoin traded that day.\n",
      "TWAP:The time-weighted average price of Bitcoin traded that day.\n",
      "\n",
      "Step 3: Understand the data.\n",
      "One of the most common reasons to wrangle data is when there's \"too much\" information packed into a single table, especially when dealing with time series data.\n",
      "Generally, all observations should be equivalent in granularity and inunits.\n",
      "There will be exceptions, but for the most part, this rule of thumb can save you from many headaches.\n",
      "\n",
      "Equivalence in Granularity -For example, you could have 10 rows of data from 10 different cryptocurrencies. However, you shouldnothave an 11th row with average or total values from the other 10 rows. That 11th row would be an aggregation, and thus not equivalent in granularity to the other 10.\n",
      "Equivalence in Units - You could have 10 rows with prices in USD collected at different dates. However, you shouldnotthen have another 10 rows with prices quoted in EUR. Any aggregations, distributions, visualizations, or statistics would become meaningless.\n",
      "\n",
      "Our current raw dataset breaks both of these rules!\n",
      "Data stored in CSV files or databases are often in stacked or record format. They use a single\r\n",
      "\t\t\t'Code'column as a catch-all for metadata. For example, in the sample dataset, we have the follow codes:\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tUnique codes in the dataset\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t123456789\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Unique codes in the datasetprint( df.Code.unique() )# ['GWA_BTC' 'GWA_ETH' 'GWA_LTC' 'GWA_XLM' 'GWA_XRP' 'MWA_BTC_CNY'#'MWA_BTC_EUR' 'MWA_BTC_GBP' 'MWA_BTC_JPY' 'MWA_BTC_USD' 'MWA_ETH_CNY'#'MWA_ETH_EUR' 'MWA_ETH_GBP' 'MWA_ETH_JPY' 'MWA_ETH_USD' 'MWA_LTC_CNY'#'MWA_LTC_EUR' 'MWA_LTC_GBP' 'MWA_LTC_JPY' 'MWA_LTC_USD' 'MWA_XLM_CNY'#'MWA_XLM_EUR' 'MWA_XLM_USD' 'MWA_XRP_CNY' 'MWA_XRP_EUR' 'MWA_XRP_GBP'#'MWA_XRP_JPY' 'MWA_XRP_USD']\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\tFirst, see how some codes start with GWA and others with MWA? These are actually completely different types of indicators according to the documentation page.\n",
      "\n",
      "MWA stands for \"market-weighted average,\" and they show regional prices. There are multiple MWA codes for each cryptocurrency, one for each local fiat currency.\n",
      "On the other hand, GWA stands for \"global-weighted average,\" which shows globally indexed prices. GWA is thus an aggregation of MWA and not equivalent in granularity. (Note: only a subset of regional MWA codes are included in the sample dataset.)\n",
      "\n",
      "For instance, let's look at Bitcoin's codes on the same date:\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tExample of GWA and MWA relationship\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t123\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Example of GWA and MWA relationshipdf[df.Code.isin(['GWA_BTC', 'MWA_BTC_JPY', 'MWA_BTC_EUR'])  & (df.Date == '2018-01-01')]\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\n",
      "            \n",
      "As you can see, we have multiple entries for a cryptocurrency on a given date. To further complicate things, the regional MWA data are denominated in their local currency (i.e. nonequivalent units), so you would also need historical exchange rates.\n",
      "Having different levels of granularity and/or different units makes analysis unwieldy at best, or downright impossible at worst.\n",
      "Luckily, once we've spotted this issue, fixing it is actually trivial!\n",
      "Step 4: Filter unwanted observations.\n",
      "One of the simplest yet most useful data wrangling techniques is removing unwanted observations.\n",
      "In the previous step, we learned that GWA codes are aggregations of the regional MWA codes. Therefore, to perform our analysis, we only need to keep the global GWA codes:\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tFilter out MWA codes\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t12345678910111213\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Number of observations in datasetprint( 'Before:', len(df) )# Before: 31761# Get all the GWA codesgwa_codes = [code for code in df.Code.unique() if 'GWA_' in code]# Only keep GWA observationsdf = df[df.Code.isin(gwa_codes)]# Number of observations leftprint( 'After:', len(df) )# After: 6309\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\tNow that we only have GWA codes left, all of our observations are equivalent in granularity and in units. We can confidently proceed.\n",
      "Step 5: Pivot the dataset.\n",
      "Next, in order to analyze our momentum trading strategy outlined above, for each cryptocurrency, we'll need calculate returns over the prior 7, 14, 21, and 28 days... for the first day of each month.\n",
      "However, it would be a huge pain to do so with the current \"stacked\" dataset. It would involve writing helper functions, loops, and plenty of conditional logic. Instead, we'll take a more elegant approach....\n",
      "First, we'll pivot the dataset while keeping only one price column. For this tutorial, let's keep theVWAP (volume weighted average price) column, but you could make a good case for most of them.\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tPivot dataset\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t12345\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Pivot datasetpivoted_df = df.pivot(index='Date', columns='Code', values='VWAP')# Display examples from pivoted datasetpivoted_df.tail()\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\n",
      "            \n",
      "As you can see, each column in our pivoted dataset now represents the price for one cryptocurrency and each row contains prices from one date. All the features are now aligned by date.\n",
      "Step 6: Shift the pivoted dataset.\n",
      "To easily calculate returns over the prior 7, 14, 21, and 28 days, we can use Pandas's shiftmethod.\n",
      "This functionshifts the indexof the dataframe by some number of periods. For example, here's what happens when we shift our pivoted dataset by 1:\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tShift method\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t12345678910111213\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\tprint( pivoted_df.tail(3) )# Code GWA_BTCGWA_ETHGWA_LTCGWA_XLMGWA_XRP# Date# 2018-01-21 12,326.23 1,108.90 197.36 0.48 1.55# 2018-01-22 11,397.52 1,038.21 184.92 0.47 1.43# 2018-01-23 10,921.00 992.05 176.95 0.47 1.42print( pivoted_df.tail(3).shift(1) )# Code GWA_BTCGWA_ETHGWA_LTCGWA_XLMGWA_XRP# Date# 2018-01-21 nannannannannan# 2018-01-22 12,326.23 1,108.90 197.36 0.48 1.55# 2018-01-23 11,397.52 1,038.21 184.92 0.47 1.43\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\tNotice how the shifted dataset now has values from 1 day before? We can take advantage of this to calculate prior returns for our 7, 14, 21, 28 day windows.\n",
      "For example, to calculate returns over the 7 days prior, we would need\r\n",
      "\t\t\tprices_today / prices_7_days_ago - 1.0, which translates to:\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tCalculate returns over 7 days prior\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t12345\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Calculate returns over 7 days priordelta_7 = pivoted_df / pivoted_df.shift(7) - 1.0# Display examplesdelta_7.tail()\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\n",
      "            \n",
      "Calculating returns for all of our windows is as easy as writing a loop and storing them in a dictionary:\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tLoop over time windows\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t1234\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Calculate returns over each window and store them in dictionarydelta_dict = {}for offset in [7, 14, 21, 28]:delta_dict['delta_{}'.format(offset)] = pivoted_df / pivoted_df.shift(offset) - 1.0\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\tNote: Calculating returns by shifting the dataset requires 2 assumptions to be met: (1) the observations are sorted ascending by date and (2) there are no missing dates. We checked this \"off-stage\" to keep this tutorial concise, but we recommend confirming this on your own.\n",
      "Step 7: Melt the shifted dataset.\n",
      "Now that we've calculated returns using the pivoted dataset, we're going to \"unpivot\" the returns. By unpivoting, ormeltingthe data, we can later create an analytical base table (ABT)where each row contains all of the relevant information for a particular coin on a particular date.\n",
      "We couldn't directly shift the original dataset because the data for different coins were stacked on each other, so the boundaries would've overlapped. In other words, BTC data would leak into ETH calculations, ETH data would leak into LTC calculations, and so on.\n",
      "To melt the data, we'll...\n",
      "\n",
      "\r\n",
      "\t\t\treset_index()so we can call the columns by name.\n",
      "Call the\r\n",
      "\t\t\tmelt()method.\n",
      "Pass the column(s) to keep into the\r\n",
      "\t\t\tid_vars=argument.\n",
      "Name the melted column using the\r\n",
      "\t\t\tvalue_name=argument.\n",
      "\n",
      "Here's how that looks for one dataframe:\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tMelt shifted data\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t12345\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Melt delta_7 returnsmelted_7 = delta_7.reset_index().melt(id_vars=['Date'], value_name='delta_7')# Melted dataframe examplesmelted_7.tail()\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\n",
      "            \n",
      "To do so for all of the returns dataframes, we can simply loop through\r\n",
      "\t\t\tdelta_dict, like so:\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t# Melt all the delta dataframes\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t1234\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Melt all the delta dataframes and store in listmelted_dfs = []for key, delta_df in delta_dict.items():melted_dfs.append( delta_df.reset_index().melt(id_vars=['Date'], value_name=key) )\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\tFinally, we can create another melted dataframe that contains the forward-looking 7-day returns. This will be our \"target variable\" for evaluating our trading strategy.\n",
      "Simply shift the pivoted dataset by \r\n",
      "\t\t\t-7 to get \"future\" prices, like so:\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tCalculate forward-looking 7-day returns\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t12345\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Calculate 7-day returns after the datereturn_df = pivoted_df.shift(-7) / pivoted_df - 1.0# Melt the return dataset and append to listmelted_dfs.append( return_df.reset_index().melt(id_vars=['Date'], value_name='return_7') )\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\tWe now have 5 melted dataframes stored in the\r\n",
      "\t\t\tmelted_dfslist, one for each of the backward-looking 7, 14, 21, and 28-day returns and one for the forward-looking 7-day returns.\n",
      "Step 8: Reduce-merge the melted data.\n",
      "All that's left to do is join our melted dataframes into a single analytical base table. We'll need two tools.\n",
      "The first is Pandas's merge function, which works like SQL JOIN. For example, to merge the first two melted dataframes...\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tMerge two dataframes\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t12\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Merge two dataframespd.merge(melted_dfs[0], melted_dfs[1], on=['Date', 'Code']).tail()\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\n",
      "            \n",
      "See how we now havedelta_7 anddelta_14 in the same row? This is the start of our analytical base table. All we need to do now is merge all of our melted dataframes together with a base dataframe of other features we might want.\n",
      "The most elegant way to do this is using Python's built-inreduce function. First we'll need to import it:\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tImport reduce function\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t1\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\tfrom functools import reduce\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\tNext, before we use that function, let's create a\r\n",
      "\t\t\tfeature_dfslist that contains base features from the original dataset plus the melted datasets.\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tCreate feature_dfs list\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t12345\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Grab features from original datasetbase_df = df[['Date', 'Code', 'Volume', 'VWAP']]# Create a list with all the feature dataframesfeature_dfs = [base_df] + melted_dfs\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\tNow we're ready to use the reduce function. Reduce applies a function of two arguments cumulatively to the objects in a sequence (e.g. a list). For example,\r\n",
      "\t\t\treduce(lambda x,y: x+y, [1,2,3,4,5])calculates\r\n",
      "\t\t\t((((1+2)+3)+4)+5).\n",
      "Thus, we can reduce-merge all of the features like so:\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tReduce-merge features into ABT\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t12345\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Reduce-merge features into analytical base tableabt = reduce(lambda left,right: pd.merge(left,right,on=['Date', 'Code']), feature_dfs)# Display examples from the ABTabt.tail(10)\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\n",
      "            \n",
      "Data Dictionary for our Analytical Base Table (ABT):\n",
      "\n",
      "Date:The day on which the index values were calculated.\n",
      "Code:Which cryptocurrency.\n",
      "VWAP:The volume weighted average price traded that day.\n",
      "delta_7:Return over the prior 7 days (1.0 = 100% return).\n",
      "delta_14:Return over the prior 14 days (1.0 = 100% return).\n",
      "delta_21:Return over the prior 21 days (1.0 = 100% return).\n",
      "delta_28:Return over the prior 28 days (1.0 = 100% return).\n",
      "return_7:Future return over the next 7 days (1.0 = 100% return).\n",
      "\n",
      "By the way, notice how the last 7 observations don't have values for the\r\n",
      "\t\t\t'return_7'feature? This is expected, as we cannot calculate \"future 7-day returns\" for the last 7 days of the dataset.\n",
      "Technically, with this ABT, we can already answer our original objective. For example, if we wanted to pick the coin that had the biggest momentum on September 1st, 2017, we could simply display the rows for that date and look at the 7, 14, 21, and 28-day prior returns:\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t# Data from Sept 1st, 2017\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t12\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Data from Sept 1st, 2017abt[abt.Date == '2017-09-01']\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\n",
      "            \n",
      "And if you wanted to programmatically pick the crypto with the biggest momentum (e.g. over the prior 28 days), you would write:\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tProgrammatically pick highest momentum\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t12345\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\tmax_momentum_id = abt[abt.Date == '2017-09-01'].delta_28.idxmax()daily_df.loc[max_momentum_id, ['Code','return_7']]# CodeGWA_LTC# return_7-0.10# Name: 3543, dtype: object\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\tHowever, since we're only interested in trading on the first day of each month, we can make things even easier for ourselves...\n",
      "Step 9: (Optional) Aggregate with group-by.\n",
      "As a final step, if we wanted to only keep the first days of each month, we can use agroup-byfollowed by an aggregation.\n",
      "\n",
      "First, create a new \r\n",
      "\t\t\t'month' feature from the first 7 characters of the Date strings.\n",
      "Then, group the observations by\r\n",
      "\t\t\t'Code'and by\r\n",
      "\t\t\t'month'. Pandas will create \"cells\" of data that separate observations by Code and month.\n",
      "Finally, within each group, simply take the\r\n",
      "\t\t\t.first()observation and reset the index.\n",
      "\n",
      "Note: We're assuming your dataframe is still properly sorted by date.\n",
      "Here's what it looks like all put together:\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tAggregate with group-by\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t12345678\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# Create 'month' featureabt['month'] = abt.Date.apply(lambda x: x[:7])# Group by 'Code' and 'month' and keep first dategb_df = abt.groupby(['Code', 'month']).first().reset_index()# Display examplesgb_df.tail()\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\n",
      "            \n",
      "As you can see, we now have a proper ABT with:\n",
      "\n",
      "Only relevant data from the 1st day of each month.\n",
      "Momentum features calculated from the prior 7, 14, 21, and 28 days.\n",
      "The future returns you would've made 7 days later.\n",
      "\n",
      "In other words, we have exactly what we need to evaluate the simple trading strategy we proposed at the beginning!\n",
      "\n",
      "Congratulations... you've made it to the end of this Python data wrangling tutorial!\n",
      "We introduced several key tools for filtering, manipulating, and transforming datasets in Python, but we've only scratched the surface. Pandas is a very powerful library with plenty of additional functionality.\n",
      "For continued learning, we recommend downloading more datasetsfor hands-on practice. Propose an interesting question, plan your approach, and fall back on documentation for help.\n",
      "We also provide over-the-shoulder guidance in our popular Machine Learning Masterclass.It's a hands-on course developed completely in-house... and it's designed take you from 0 to machine learning as smoothly as possible (without the boring lectures).\n",
      "The complete code, from start to finish.\n",
      "Here'sall the main code in one place, in a single script.\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\tComplete script\r\n",
      "\t\t\tPython\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t1234567891011121314151617181920212223242526272829303132333435363738394041\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t\t# 2. Import libraries and datasetimport pandas as pdpd.options.display.float_format = '{:,.2f}'.formatpd.options.display.max_rows = 200pd.options.display.max_columns = 100df = pd.read_csv('BNC2_sample.csv', names=['Code', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'TWAP'])# 4. Filter unwanted observationsgwa_codes = [code for code in df.Code.unique() if 'GWA_' in code]df = df[df.Code.isin(gwa_codes)]# 5. Pivot the datasetpivoted_df = df.pivot(index='Date', columns='Code', values='VWAP')# 6. Shift the pivoted datasetdelta_dict = {}for offset in [7, 14, 21, 28]:delta_dict['delta_{}'.format(offset)] = pivoted_df / pivoted_df.shift(offset) - 1# 7. Melt the shifted datasetmelted_dfs = []for key, delta_df in delta_dict.items():melted_dfs.append( delta_df.reset_index().melt(id_vars=['Date'], value_name=key) )return_df = pivoted_df.shift(-7) / pivoted_df - 1.0melted_dfs.append( return_df.reset_index().melt(id_vars=['Date'], value_name='return_7') )# 8. Reduce-merge the melted datafrom functools import reducebase_df = df[['Date', 'Code', 'Volume', 'VWAP']]feature_dfs = [base_df] + melted_dfsabt = reduce(lambda left,right: pd.merge(left,right,on=['Date', 'Code']), feature_dfs)# 9. Aggregate with group-by.abt['month'] = abt.Date.apply(lambda x: x[:7])gb_df = abt.groupby(['Code', 'month']).first().reset_index()\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\n",
      "58SHARESShareGoogleLinkedinTweet\r\n",
      "            \r\n",
      "                \r\n",
      "            \n",
      "\n",
      "\n",
      "                \n",
      "                    \n",
      "                    \n",
      "                        \n",
      "                             Previous Post\n",
      "                            How to Become a Data Scientist, The Self-Starter Way\n",
      "                        \n",
      "                        \n",
      "                            Next Post \n",
      "                            Python for Data Science (Ultimate Quickstart Guide)\n"
     ]
    }
   ],
   "source": [
    "post_file = open(\"Data/\"+post_title.text_content().split(':')[0]+\".txt\",'w')\n",
    "text = safeStr(post_content.text_content().strip())\n",
    "print text\n",
    "post_file.write(text)\n",
    "post_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
